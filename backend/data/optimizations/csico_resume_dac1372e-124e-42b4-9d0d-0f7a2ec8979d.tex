\documentclass[letterpaper,11pt]{article}
\usepackage{fontawesome}
\usepackage{latexsym}
\usepackage[empty]{fullpage}
\usepackage{titlesec}
\usepackage{marvosym}
\usepackage[usenames,dvipsnames]{color}
\usepackage{verbatim}
\usepackage{enumitem}
\usepackage[hidelinks]{hyperref}
\usepackage{fancyhdr}
\usepackage[english]{babel}
\usepackage{tabularx}
\usepackage{adjustbox}
\usepackage{ragged2e}
\usepackage{changepage}

\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

\addtolength{\oddsidemargin}{-0.5in}
\addtolength{\evensidemargin}{-0.5in}
\addtolength{\textwidth}{1in}
\addtolength{\topmargin}{-0.5in}
\addtolength{\textheight}{1.0in}

\urlstyle{same}

\setlength{\parindent}{0pt}
\setlength{\parskip}{1pt}
\setlength{\tabcolsep}{0in}

\titleformat{\section}{
\vspace{-4pt}\scshape\raggedright\large
}{}{0em}{}[\color{black}\titlerule \vspace{-4pt}]

\pdfgentounicode=1

%-------------------------
% Custom Commands
%-------------------------
\newcommand{\resumeItem}[1]{
\begin{tabularx}{\linewidth}{@{}lX@{}}
\raisebox{0.25ex}{\tiny\textbullet} & \small #1 \\
\end{tabularx}
}
\newcommand{\resumeSubheading}[4]{\vspace{-2pt}\item
\begin{tabularx}{\linewidth}{@{}X@{\hfill}r@{}}
\textbf{#1} & #2 \\
{\small #3} & {\small #4} \\
\end{tabularx}
\vspace{-6pt}
}
\newcommand{\resumeProjectHeading}[2]{\item
\begin{tabularx}{\linewidth}{@{}X@{\hfill}r@{}}
\textbf{#1} & #2 \\
\end{tabularx}
\vspace{-6pt}
}
\newcommand{\resumeSubItem}[1]{\resumeItem{#1}\vspace{-4pt}}
\renewcommand\labelitemii{$\vcenter{\hbox{\tiny$\bullet$}}$}
\newcommand{\resumeSubHeadingListStart}{\begin{itemize}[leftmargin=0in, label={}]} 
\newcommand{\resumeSubHeadingListEnd}{\end{itemize}}
\newcommand{\resumeItemListStart}{\begin{itemize}[leftmargin=*, itemsep=2pt, topsep=2pt, parsep=0pt]}
\newcommand{\resumeItemListEnd}{\end{itemize}\vspace{-5pt}}

%-------------------------
% Document
%-------------------------
\begin{document}

%-----------Header-----------
\begin{center}
{\Huge \scshape Dharama Teja Samudrala} \\ \vspace{2pt} 
New York, NY 14221 \\ \vspace{2pt}
\small
+18573983456 $|$ \href{mailto:dharmatejas102@gmail.com}{dharmatejas102@gmail.com} $|$ 
\href{https://www.linkedin.com/in/dharmatejasamudrala/}{linkedin} $|$ 
\href{https://github.com/dharmateja03}{github}
\end{center}

%-----------Summary-----------
\section{Summary}
\textbf{Data Engineer} with 3+ years of experience building scalable data pipelines and delivering insights across \textbf{healthcare, finance, and insurance} domains. Skilled in \textbf{Python, SQL, Spark, ETL, cloud data platforms,} and \textbf{natural language processing}. Passionate about leveraging data and AI/ML to drive business value. Proven ability to collaborate with cross-functional teams and communicate technical concepts effectively.

%-----------Experience-----------
\section{Experience}
\resumeSubHeadingListStart
\resumeSubheading{Citi}{NewYork,USA}
{Data Engineer - Financial Analytics Platform}{March 2025 -- Present}
\resumeItemListStart
\resumeItem{Designed and deployed ETL pipelines using \textbf{Airflow, PySpark,} and \textbf{AWS Glue}, ingesting data from 10+ sources and reducing manual processing time by 40\% for a 5-member team.}
\resumeItem{Built streaming infrastructure with \textbf{Kafka} and \textbf{Flume} to process 500,000 financial transactions per day with sub-second latency, enhancing real-time analytics capabilities.}
\resumeItem{Optimized data storage and querying on \textbf{Amazon S3, Redshift, Snowflake}, reducing query latency by 30\% and costs by 25\% through partitioning and compression techniques.}
\resumeItem{Implemented CI/CD and monitoring using \textbf{Jenkins, AWS CodePipeline, Airflow}, automating data quality checks that caught 90\% of anomalies pre-deployment.}
\resumeItemListEnd

\resumeSubheading{Fidelity Investments}{Raleigh, NC, USA}
{Data Engineer Intern - Investment Analytics}{Jun 2024 -- Aug 2024}
\resumeItemListStart
\resumeItem{Optimized \textbf{PySpark} financial processing workflows on \textbf{AWS EMR}, reducing execution time by 55\% for 10 GB datasets while maintaining data integrity.
}
\resumeItem{Evaluated \textbf{AWS Neptune Graph Database} for team workflow analysis, conducting performance benchmarking and data modeling.}
\resumeItem{Implemented serverless APIs using \textbf{AWS API Gateway} and \textbf{Lambda}, enabling secure access to investment analytics for internal clients.}
\resumeItem{Developed and maintained \textbf{Airflow} DAGs for financial data pipelines, ensuring proper task dependencies and error handling.}
\resumeItemListEnd

\resumeSubheading{MetLife}{India}
{Data Engineer}{Dec 2021 -- Jul 2023}
\resumeItemListStart
\resumeItem{Managed \textbf{Amazon S3} buckets storing 2 TB of insurance data; implemented partitioning to optimize \textbf{Athena} queries, reducing runtime by 20\%.}
\resumeItem{Built and maintained \textbf{MySQL, PostgreSQL, MongoDB} databases supporting analytics for 120,000 daily customer records.}
\resumeItem{Developed \textbf{PySpark ETL} jobs to process 3 GB of transactional data per day from multiple \textbf{AWS} sources.}
\resumeItem{Designed \textbf{Airflow} workflows to orchestrate and monitor 8 recurring ETL pipelines, ensuring 99\% uptime.}
\resumeItemListEnd
\vspace{-5mm}

\resumeSubheading{Catalog}{India}
{Machine Learning Intern}{Jan 2021 -- Jun 2021}
\resumeItemListStart
\resumeItem{Conducted distributed data analysis using \textbf{PySpark} to validate ML model scalability on large blockchain datasets.}
\resumeItem{Developed \textbf{Python} and \textbf{SQL ETL} pipelines to preprocess raw data for model training workflows.}
\resumeItem{Automated model training and evaluation with \textbf{Airflow} DAGs and deployed microservices using \textbf{FastAPI}.}
\resumeItemListEnd

\resumeSubHeadingListEnd

%-----------Technical Skills-----------
\section{Technical Skills}
\begin{tabularx}{\textwidth}{@{}lX@{}}
\textbf{Languages:} & \textbf{Python}, \textbf{SQL}, R, \textbf{Scala} \\
\textbf{Data Engineering:} & \textbf{Spark}, \textbf{Airflow}, Kafka, \textbf{Snowflake}, AWS (S3, EMR, Glue, Lambda), Azure, \textbf{Docker} \\
\textbf{Databases:} & \textbf{MySQL}, \textbf{PostgreSQL}, \textbf{MongoDB}, \textbf{Redshift}, Cassandra \\
\textbf{Machine Learning:} & PyTorch, scikit-learn, \textbf{Hugging Face}, \textbf{Jupyter}, MLflow, FastAPI \\
\textbf{Tools \& Technologies:} & \textbf{Git}, \textbf{Jenkins}, Prometheus, Grafana, Tableau, Unix/Linux
\end{tabularx}

%-----------Projects-----------
\section{Projects}
\resumeSubHeadingListStart
\resumeProjectHeading{\textbf{Generative AI Content Assistant | Python, PyTorch, Hugging Face, AWS}}{}
\resumeItemListStart
\resumeItem{Developed a generative AI application to assist content creators using \textbf{GPT-3} and \textbf{Hugging Face} models, serving as the lead engineer in a 3-member team.}
\resumeItem{Fine-tuned language models on domain-specific datasets using \textbf{PyTorch} and implemented \textbf{text summarization}, \textbf{entity extraction}, and \textbf{sentiment analysis} pipelines.}
\resumeItem{Deployed the application on \textbf{AWS EC2} and used \textbf{SageMaker} for model hosting, enabling support for 10,000 monthly active users.}
\resumeItem{Reduced content creation time by 30\% and improved engagement metrics by 20\% for users of the AI writing assistant.}
\resumeItemListEnd

\resumeProjectHeading{\textbf{Predictive Maintenance Platform | PySpark, Kafka, Flask}}{}
\resumeItemListStart
\resumeItem{Built a predictive maintenance platform to monitor industrial equipment sensor data using \textbf{PySpark}, \textbf{Kafka}, and \textbf{scikit-learn}, working independently on end-to-end development.}
\resumeItem{Ingested real-time sensor data from 1,000 devices using \textbf{Kafka} streams and performed feature engineering using \textbf{PySpark} on 10 GB datasets.}
\resumeItem{Trained machine learning models to predict equipment failures and optimized hyperparameters using \textbf{MLflow}, achieving 85\% recall.}
\resumeItem{Created a web dashboard with \textbf{Flask} to visualize real-time predictions and alerts, reducing unplanned downtime by 20\%.}
\resumeItemListEnd

\resumeProjectHeading{\textbf{Customer Churn Analysis | Snowflake, dbt, Tensorflow}}{}
\resumeItemListStart
\resumeItem{Conducted churn analysis on a 50 GB customer dataset using \textbf{Snowflake}, \textbf{dbt}, and \textbf{Tensorflow}, collaborating with a data scientist on model development.}
\resumeItem{Built \textbf{ETL} pipelines in \textbf{dbt} to transform raw data into features for model training, ensuring data quality and consistency.}
\resumeItem{Trained a \textbf{neural network} model using \textbf{Tensorflow} to predict churn likelihood, achieving an AUROC of 0.89 on validation data.}
\resumeItem{Provided actionable insights to business stakeholders for churn prevention strategies, potentially saving \$500K in annual revenue.}
\resumeItemListEnd

\resumeSubHeadingListEnd

%-----------Education-----------
\section{Education}
\resumeSubHeadingListStart
\resumeSubheading{Rochester Institute Of Technology}{Rochester, NY, USA}
{Master of Science in Computer Science}{Aug 2023 -- Aug 2025}
\resumeSubHeadingListEnd

\end{document}