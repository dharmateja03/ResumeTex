Problem: optimizing resume based on job description (latex file primarily )

Solution Use llm agent 


 What Resume Optimizer Does
  Core Function: Takes a user's LaTeX resume (.tex file) and optimizes it for specific job descriptions using AI language models, then generates an improved PDF resume.
  Architecture Overview
  Frontend (React/Vite) ←→ Backend (FastAPI) ←→ Database (SQLite) ←→ LLM APIs ←→ LaTeX Compiler
  Tech Stack
  Frontend
  - React 18 with TypeScript
  - Vite (dev server on port 5176)
  - React Router for navigation
  - Tailwind CSS for styling
  - Lucide React for icons
  - Custom fonts: Gambarino, Switzer
  Backend
  - FastAPI (Python web framework)
  - Uvicorn (ASGI server on port 8000)
  - SQLAlchemy (ORM)
  - SQLite (database)
  - Pydantic (data validation)
  - JWT authentication
  - Passlib (password hashing)
  - python-multipart (file uploads)
  AI Integration
  - Multiple LLM providers: OpenAI, Anthropic, Google, Mistral, Custom
  - Dynamic model selection
  - Configurable API integration
  LaTeX Processing
  - Tectonic or pdflatex (LaTeX compilers)
  - LaTeX validation and cleaning
  - PDF generation
  - Content structure analysis
  Detailed Process Flow
  1. User Authentication
  Login → JWT Token → Development Bypass (dev-token-*) → Admin User Creation
  2. Resume Upload
  .tex file → Validation → LaTeX parsing → Database storage → Memory cache
  3. Job Optimization
  Job URL/Description → Web scraping → LLM processing → LaTeX modification → PDF generation
  4. LLM Configuration
  Provider selection → Model selection → Configuration save → Default setting
  Database Schema
  Core Tables
  - Users: Authentication, roles (master/admin/employee)
  - UserLLMConfig: LLM provider configurations
  - Optimizations: Job optimization history
  - AdminConfig: System configurations
  - APIUsage: Usage tracking
  - Subscriptions: Payment/subscription data
  - PaymentHistory: Transaction records
  Key Features
  Resume Management
  - Upload: LaTeX file validation and storage
  - Storage: Database + in-memory caching
  - Validation: LaTeX syntax checking
  - Processing: Content extraction and analysis
  Job Optimization
  - Input methods: Job URL scraping or direct description
  - AI processing: Content optimization using configured LLMs
  - Output: Modified LaTeX + compiled PDF
  - Location customization: Dynamic address replacement
  - Keyword optimization: Job-specific tailoring
  LLM Integration
  - Multi-provider support: OpenAI, Anthropic, Google AI, Mistral
  - Model selection: Dynamic model listing per provider
  - Configuration management: Save/load user preferences
  - Default settings: Primary LLM selection
  - API abstraction: Unified interface for different providers
  Authentication System
  - JWT-based: Secure token authentication
  - Role-based: Master/Admin/Employee permissions
  - Development mode: Fake token bypass (dev-token-*)
  - Session management: LocalStorage token persistence
  File Structure
  src/
  ├── pages/
  │   ├── Login.tsx          # Google SSO + dev login
  │   ├── Dashboard.tsx      # File upload + quick actions
  │   ├── LLMSettings.tsx    # LLM configuration
  │   ├── Optimize.tsx       # Job optimization interface
  │   └── PersonalApp.tsx    # Personal route landing
  ├── components/
  │   └── PersonalNavbar.tsx # Personal app navigation
  ├── AppRouter.tsx          # Route configuration
  └── index.tsx             # App entry point
  backend/
  ├── main1.py              # FastAPI main application
  ├── auth.py               # JWT authentication
  ├── database.py           # SQLAlchemy models
  ├── llm_handlers.py       # LLM provider integrations
  ├── personal_app.py       # Personal app endpoints
  └── startup.py            # Database initialization
  API Endpoints
  Authentication
  - POST /register - User registration
  - POST /login - JWT login
  - GET /user-llm-configs - Get user configurations
  Resume Operations
  - POST /upload_master_resume/ - Upload LaTeX file
  - POST /optimize_resume - Optimize for job
  - GET /download_temp_file - Download generated PDF
  LLM Management
  - GET /available-llms - List providers/models
  - POST /save-llm-config - Save LLM configuration
  - GET /user-llm-configs - Get saved configurations
  System
  - POST /test-latex-compilation - Test LaTeX setup
  Development Setup
  - Frontend: npm run dev (port 5176)
  - Backend: python main1.py (port 8000)
  - Proxy: Vite proxies API calls to backend
  - Auth bypass: Development tokens for testing
  - Hot reload: Both frontend and backend support live updates
  Key Dependencies
  Frontend: react, typescript, vite, tailwindcss, react-router-dom
  Backend: fastapi, sqlalchemy, pydantic, passlib, python-jose, tectonic
  This system provides a complete resume optimization workflow with modern web architecture, AI integration, and professional PDF generation capabilities.
> what about reddis and promt.txt file
⏺ You're absolutely right! I missed some key components. Let me check for Redis and prompt files:
⏺ Search(pattern: "redis")
  ⎿  Found 0 files 
⏺ Search(pattern: "*redis*")
  ⎿  Found 4 files (ctrl+r to expand)
⏺ Search(pattern: "*prompt*")
  ⎿  Found 1 file (ctrl+r to expand)
⏺ Search(pattern: "*celery*")
  ⎿  Found 1 file (ctrl+r to expand)
⏺ Perfect! There are additional components I missed. Let me examine them:
⏺ Read(redis_config.py)
  ⎿  Read 192 lines (ctrl+r to expand)
⏺ Read(latex_system_prompt.txt)
  ⎿  Read 18 lines (ctrl+r to expand)
⏺ Read(celery_tasks.py)
  ⎿  Read 76 lines (ctrl+r to expand)
⏺ Read(REDIS_SETUP.md)
  ⎿  Read 178 lines (ctrl+r to expand)
⏺ Perfect! You're absolutely right. Here's the COMPLETE and ACCURATE Resume Optimizer architecture including Redis and prompts:
  Updated Complete Architecture
  Frontend (React) ←→ Backend (FastAPI) ←→ Redis Cache ←→ Celery Workers ←→ LLM APIs
                                      ↓
                                Database (SQLite)
                                      ↓
                              LaTeX Compiler (Tectonic/pdflatex)
  Additional Tech Stack Components I Missed
  Caching & Queue System
  - Redis (v5.0.1) - Distributed caching and message broker
  - Celery (v5.3.4) - Async task processing
  - In-memory fallback - When Redis unavailable
  AI Prompting System
  - LaTeX system prompt (latex_system_prompt.txt) - LLM instructions for resume optimization
  Redis Architecture Details
  Storage Patterns
  # Redis Key Structure:
  resume_meta:<user_id>:<session_id>     # Metadata (24h TTL)
  resume_content:<cache_key>             # Large .tex files >3KB (24h TTL)  
  pdf_result:<cache_key>:<company_name>  # Generated PDFs (1h TTL)
  Content Size Logic
  - Small files (<3KB): Stored in session cookies
  - Large files (>3KB): Stored in Redis with MD5 hash keys
  - Metadata: Always in Redis with user/session namespace
  Redis Manager Features
  - Connection handling: Auto-fallback to in-memory cache if Redis down
  - TTL management: Automatic expiration (24h content, 1h PDFs)
  - Cache key generation: MD5 hashing for content-based keys
  - Health monitoring: Connection status and cache statistics
  - Cleanup: Automatic expired content removal
  Celery Async Processing
  Task Queue Architecture
  # celery_tasks.py
  @celery_app.task(bind=True)
  def compile_latex_task(self, cache_key: str, company_name: str)
  Async Flow
  1. LLM optimization: Synchronous (fast API response)
  2. LaTeX compilation: Async via Celery (CPU-intensive)
  3. Progress tracking: Task status polling every 5 seconds
  4. PDF generation: Background processing with status updates
  Task Management
  - Task serialization: JSON format
  - Result expiration: 1 hour TTL
  - Progress states: PENDING → PROGRESS → SUCCESS/FAILURE
  - Error handling: Comprehensive exception capture
  AI Prompting System
  LaTeX System Prompt (latex_system_prompt.txt)
  CRITICAL INSTRUCTIONS:
  - Output ONLY valid, compilable LaTeX code
  - No explanations, comments, or markdown
  - Complete document from \documentclass to \end{document}
  OPTIMIZATION REQUIREMENTS:
  1. Update experience descriptions for job relevance
  2. Reorder technical skills matching job requirements
  3. Incorporate ATS-friendly keywords
  4. Maintain professional LaTeX formatting
  5. Ensure proper page fitting
  Prompt Engineering
  - Strict output format: Pure LaTeX only
  - ATS optimization: Keyword incorporation strategy
  - Structure preservation: Maintains resume formatting
  - Error handling: Specific error messages for missing templates
  Updated Process Flow
  1. Resume Upload
  .tex file → Size check → Redis storage (>3KB) OR Session storage (<3KB)
           → Metadata storage → Database record → Cache key generation
  2. Job Optimization (Async)
  Job description → LLM processing (sync) → Optimized LaTeX → Redis storage
                                        → Celery task queue → PDF compilation (async)
                                        → Status polling → PDF download
  3. Caching Strategy
  Session data: Small metadata, user preferences
  Redis: Large .tex content, optimized results, PDF paths
  Database: User accounts, configurations, history
  Production Architecture Benefits
  Scalability
  - Horizontal scaling: Multiple FastAPI instances share Redis
  - Load distribution: Celery workers handle CPU-intensive tasks
  - Session sharing: Users can switch between server instances
  - Memory efficiency: Large files in Redis, not server RAM
  Reliability
  - Fallback caching: In-memory cache when Redis unavailable
  - Task persistence: Celery ensures PDF generation completion
  - TTL management: Automatic cleanup prevents cache bloat
  - Health monitoring: Redis connection status tracking
  Performance
  - Non-blocking: LaTeX compilation doesn't freeze web server
  - Concurrent processing: Multiple PDF generations simultaneously
  - Smart caching: Content-based keys prevent duplicate processing
  - Background tasks: CPU-intensive work moved to worker processes
  Updated File Structure
  src/ (Frontend - React/TypeScript)
  ├── pages/LLMSettings.tsx    # LLM configuration
  ├── pages/Dashboard.tsx      # File upload + Redis integration
  └── AppRouter.tsx            # Route management
  backend/ (Python FastAPI)
  ├── main1.py                 # Main FastAPI app
  ├── redis_config.py          # Redis manager with fallback
  ├── celery_tasks.py          # Async task processing
  ├── latex_system_prompt.txt  # LLM optimization instructions
  ├── personal_app.py          # Personal app with Redis integration
  ├── auth.py                  # JWT + development bypass
  └── llm_handlers.py          # Multi-provider LLM integration
  monitoring/
  ├── REDIS_SETUP.md          # Architecture documentation
  ├── REDIS_FALLBACK_COMPLETE.md  # Fallback strategy
  └── test_redis.py           # Redis connection testing
  Environment Setup
  # Redis (required for production)
  REDIS_URL=redis://localhost:6379/0
  # Celery worker startup
  celery -A celery_tasks worker --loglevel=info
  # FastAPI with Redis integration
  SECRET_KEY="dev-secret" python main1.py
  This is the complete, distributed architecture with Redis caching, Celery async processing, sophisticated prompting system, and production-ready scalability features.



Pages
/index page->main page
/login ->login with google
/llm-settings->give llm and api keys 
/optimize->optimiazer and give pdf of optimized output



User logins using google sso->check browsers cache->if llm setting found okay->once logins ask fo lllm settings ->once given shold check connection before saving once conenction is okay then save 



